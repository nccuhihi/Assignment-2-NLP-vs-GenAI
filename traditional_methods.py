# ==========================================
# æª”å: traditional_methods.py
# ==========================================

# 1. å®‰è£èˆ‡åŒ¯å…¥å¿…è¦å¥—ä»¶
try:
    import jieba
    import pandas as pd
    import numpy as np
except ImportError:
    print("æ­£åœ¨å®‰è£ Part A å¿…è¦å¥—ä»¶...")
    !pip install -q jieba pandas numpy
    import jieba
    import pandas as pd
    import numpy as np

import math
import re
import sys
from collections import Counter

# 2. é¡¯ç¤ºå¥—ä»¶ç‰ˆæœ¬
print("=== Part A ç’°å¢ƒæª¢æŸ¥ ===")
print(f"Python version: {sys.version.split()[0]}")
print(f"jieba version: {jieba.__version__}")
print(f"pandas version: {pd.__version__}")
print(f"numpy version: {np.__version__}")
print("=======================\n")

# 3. å®šç¾©å…±ç”¨è³‡æ–™ (ç¨ç«‹å­˜åœ¨æ–¼æ­¤æª”æ¡ˆ)
documents = [
    "äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šä¸–ç•Œ,æ©Ÿå™¨å­¸ç¿’æ˜¯å…¶æ ¸å¿ƒæŠ€è¡“",
    "æ·±åº¦å­¸ç¿’æ¨å‹•äº†äººå·¥æ™ºæ…§çš„ç™¼å±•,ç‰¹åˆ¥æ˜¯åœ¨åœ–åƒè­˜åˆ¥é ˜åŸŸ",
    "ä»Šå¤©å¤©æ°£å¾ˆå¥½,é©åˆå‡ºå»é‹å‹•",
    "æ©Ÿå™¨å­¸ç¿’å’Œæ·±åº¦å­¸ç¿’éƒ½æ˜¯äººå·¥æ™ºæ…§çš„é‡è¦åˆ†æ”¯",
    "é‹å‹•æœ‰ç›Šå¥åº·,æ¯å¤©éƒ½æ‡‰è©²ä¿æŒé‹å‹•ç¿’æ…£"
]

test_texts = [
    "é€™å®¶é¤å»³çš„ç‰›è‚‰éºµçœŸçš„å¤ªå¥½åƒäº†,æ¹¯é ­æ¿ƒéƒ,éºµæ¢Qå½ˆ,ä¸‹æ¬¡ä¸€å®šå†ä¾†!",
    "æœ€æ–°çš„AIæŠ€è¡“çªç ´è®“äººé©šè‰·,æ·±åº¦å­¸ç¿’æ¨¡å‹çš„è¡¨ç¾è¶Šä¾†è¶Šå¥½",
    "é€™éƒ¨é›»å½±åŠ‡æƒ…ç©ºæ´,æ¼”æŠ€ç³Ÿç³•,å®Œå…¨æ˜¯æµªè²»æ™‚é–“",
    "æ¯å¤©æ…¢è·‘5å…¬é‡Œ,é…åˆé©ç•¶çš„é‡è¨“,é«”èƒ½é€²æ­¥å¾ˆå¤š"
]

long_text_example = """
äººå·¥æ™ºæ…§ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œå®ƒè©¦åœ–äº†è§£æ™ºèƒ½çš„å¯¦è³ªï¼Œä¸¦ç”Ÿç”¢å‡ºä¸€ç¨®æ–°çš„èƒ½ä»¥äººé¡æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºåæ‡‰çš„æ™ºèƒ½æ©Ÿå™¨ã€‚
äººå·¥æ™ºæ…§çš„ç ”ç©¶é ˜åŸŸä¸»è¦åŒ…æ‹¬æ©Ÿå™¨äººã€èªè¨€è­˜åˆ¥ã€åœ–åƒè­˜åˆ¥ã€è‡ªç„¶èªè¨€è™•ç†å’Œå°ˆå®¶ç³»çµ±ç­‰ã€‚
è‡ªå¾äººå·¥æ™ºæ…§èª•ç”Ÿä»¥ä¾†ï¼Œç†è«–å’ŒæŠ€è¡“æ—¥ç›Šæˆç†Ÿï¼Œæ‡‰ç”¨é ˜åŸŸä¹Ÿä¸æ–·æ“´å¤§ã€‚
å¯ä»¥è¨­æƒ³ï¼Œæœªä¾†äººå·¥æ™ºæ…§å¸¶ä¾†çš„ç§‘æŠ€ç”¢å“ï¼Œå°‡æœƒæ˜¯äººé¡æ™ºæ…§çš„ã€Œå®¹å™¨ã€ã€‚
æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’ä¸­ä¸€ç¨®åŸºæ–¼å°æ•¸æ“šé€²è¡Œè¡¨å¾µå­¸ç¿’çš„æ¼”ç®—æ³•ã€‚
æ·±åº¦å­¸ç¿’çš„å¥½è™•æ˜¯ç”¨éç›£ç£å¼æˆ–åŠç›£ç£å¼çš„ç‰¹å¾µå­¸ç¿’å’Œåˆ†å±¤ç‰¹å¾µæå–é«˜æ•ˆç®—æ³•ä¾†æ›¿ä»£æ‰‹å·¥ç²å–ç‰¹å¾µã€‚
"""

print("=== Part A: å‚³çµ± NLP æ–¹æ³•å¯¦ä½œ ===")

# --- [A-1] æ‰‹å‹•è¨ˆç®— TF-IDF èˆ‡ ç›¸ä¼¼åº¦çŸ©é™£ ---
print("\n--- [A-1] TF-IDF é—œéµè©èˆ‡ç›¸ä¼¼åº¦çŸ©é™£ ---")

def calculate_tf(word_list):
    tf_dict = {}
    total = len(word_list)
    for w in word_list:
        tf_dict[w] = tf_dict.get(w, 0) + 1
    return {k: v/total for k, v in tf_dict.items()}

def calculate_idf(doc_list):
    idf_dict = {}
    N = len(doc_list)
    all_words = set(w for doc in doc_list for w in doc)
    for w in all_words:
        count = sum(1 for doc in doc_list if w in doc)
        idf_dict[w] = math.log(N / (count + 1)) + 1
    return idf_dict

def calculate_tfidf_similarity(doc1, doc2, corpus):
    w1, w2 = jieba.lcut(doc1), jieba.lcut(doc2)
    corpus_tokens = [jieba.lcut(d) for d in corpus]

    tf1, tf2 = calculate_tf(w1), calculate_tf(w2)
    idf = calculate_idf(corpus_tokens)
    vocab = sorted(list(set(w1) | set(w2)))

    v1, v2 = [], []
    s1, s2 = {}, {}
    for w in vocab:
        val = idf.get(w, 0)
        sc1, sc2 = tf1.get(w, 0) * val, tf2.get(w, 0) * val
        v1.append(sc1); v2.append(sc2)
        if sc1 > 0: s1[w] = sc1
        if sc2 > 0: s2[w] = sc2

    dot = sum(a*b for a,b in zip(v1, v2))
    norm_a = math.sqrt(sum(a*a for a in v1))
    norm_b = math.sqrt(sum(b*b for b in v2))
    sim = dot / (norm_a * norm_b) if norm_a > 0 and norm_b > 0 else 0.0
    return sim, s1, s2

# ç”Ÿæˆ TF-IDF ç›¸ä¼¼åº¦çŸ©é™£
print("æ­£åœ¨ç”Ÿæˆç›¸ä¼¼åº¦çŸ©é™£...")
matrix_size = len(documents)
sim_matrix = np.zeros((matrix_size, matrix_size))

for i in range(matrix_size):
    for j in range(matrix_size):
        s, _, _ = calculate_tfidf_similarity(documents[i], documents[j], documents)
        sim_matrix[i][j] = s

# è½‰ç‚º DataFrame ä¸¦å„²å­˜
df_matrix = pd.DataFrame(sim_matrix, columns=[f"Doc{i+1}" for i in range(matrix_size)],
                         index=[f"Doc{i+1}" for i in range(matrix_size)])
df_matrix.to_csv("tfidf_similarity_matrix.csv", encoding='utf-8-sig')
print("âœ… å·²ç”Ÿæˆ 'tfidf_similarity_matrix.csv'")
print(df_matrix)


sim_score, scores1, scores2 = calculate_tfidf_similarity(documents[0], documents[1], documents)

print(f"æ–‡æœ¬1: {documents[0]}")
print(f"æ–‡æœ¬2: {documents[1]}")
print("-" * 30)
# æ’åºä¸¦é¡¯ç¤º Top 5 é—œéµè©
print("ã€æ–‡æœ¬1 é—œéµè© TF-IDF å€¼ã€‘:")
sorted_s1 = sorted(scores1.items(), key=lambda x: x[1], reverse=True)[:5]
for word, val in sorted_s1:
    print(f"  {word}: {val:.4f}")

print("\nã€æ–‡æœ¬2 é—œéµè© TF-IDF å€¼ã€‘:")
sorted_s2 = sorted(scores2.items(), key=lambda x: x[1], reverse=True)[:5]
for word, val in sorted_s2:
    print(f"  {word}: {val:.4f}")

print("-" * 30)
print(f"ğŸ‘‰ Cosine Similarity: {sim_score:.4f}")

# --- [A-2] åŸºæ–¼è¦å‰‡çš„æ–‡æœ¬åˆ†é¡ ---
print("\n--- [A-2] åŸºæ–¼è¦å‰‡çš„æ–‡æœ¬åˆ†é¡ ---")
class RuleClassifier:
    def __init__(self):
        self.pos = {'å¥½', 'æ£’', 'å„ªç§€', 'å–œæ­¡', 'æ¨è–¦', 'æ»¿æ„', 'é©šè‰·'}
        self.neg = {'å·®', 'ç³Ÿ', 'å¤±æœ›', 'è¨å­', 'æµªè²»', 'ç„¡èŠ', 'çˆ›'}
        self.negation = {'ä¸', 'æ²’', 'ç„¡', 'é'}
        self.adv = {'å¤ª': 2.0, 'çœŸ': 1.5, 'å¾ˆ': 1.5, 'éå¸¸': 2.0}
        self.topics = {
            'ç§‘æŠ€': ['AI', 'äººå·¥æ™ºæ…§', 'é›»è…¦', 'æ¨¡å‹', 'æ·±åº¦å­¸ç¿’'],
            'é‹å‹•': ['é‹å‹•', 'å¥èº«', 'è·‘æ­¥', 'é‡è¨“', 'é«”èƒ½'],
            'ç¾é£Ÿ': ['åƒ', 'é£Ÿç‰©', 'é¤å»³', 'ç¾å‘³', 'æ–™ç†', 'ç‰›è‚‰éºµ'],
            'å¨›æ¨‚': ['é›»å½±', 'åŠ‡æƒ…', 'æ¼”æŠ€']
        }

    def analyze(self, text):
        words = jieba.lcut(text)
        score, i = 0, 0
        while i < len(words):
            w, weight, is_neg = words[i], 1.0, False
            if i>0 and words[i-1] in self.adv:
                weight = self.adv[words[i-1]]
                if i>1 and words[i-2] in self.negation: is_neg = True
            elif i>0 and words[i-1] in self.negation: is_neg = True

            val = 1 if w in self.pos else (-1 if w in self.neg else 0)
            if is_neg: val *= -1
            score += val * weight
            i += 1

        t_counts = {t: sum(1 for w in words if w in kws) for t, kws in self.topics.items()}
        return ("æ­£é¢" if score > 0 else ("è² é¢" if score < 0 else "ä¸­æ€§")), max(t_counts, key=t_counts.get)

clf = RuleClassifier()
for t in test_texts:
    s, tp = clf.analyze(t)
    print(f"æ–‡æœ¬: {t[:10]}... | æƒ…æ„Ÿ: {s} | ä¸»é¡Œ: {tp}")

# --- [A-3] çµ±è¨ˆå¼è‡ªå‹•æ‘˜è¦ ---
print("\n--- [A-3] çµ±è¨ˆå¼è‡ªå‹•æ‘˜è¦ ---")
class ManualSummarizer:
    def __init__(self):
        self.stops = set(['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'ä¹Ÿ', 'å°±', 'éƒ½'])

    def summarize(self, text, top_k=2):
        sents = [s.strip() for s in re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', text) if len(s.strip())>5]
        words = [w for s in sents for w in jieba.lcut(s) if w not in self.stops]
        freq = Counter(words)

        scores = []
        for s in sents:
            ws = [w for w in jieba.lcut(s) if w not in self.stops]
            sc = sum(freq[w] for w in ws) / (len(ws) if ws else 1)
            scores.append((sc, s))

        top = sorted(scores, key=lambda x:x[0], reverse=True)[:top_k]
        selected = [x[1] for x in top]
        return "".join([s for s in sents if s in selected])

summ = ManualSummarizer()
print(f"æ‘˜è¦çµæœ:\n{summ.summarize(long_text_example)}")
